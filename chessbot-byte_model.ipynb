{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1267593,"sourceType":"datasetVersion","datasetId":723383}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional, Union, Callable","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-14T22:18:26.689728Z","iopub.execute_input":"2024-10-14T22:18:26.690252Z","iopub.status.idle":"2024-10-14T22:18:26.696401Z","shell.execute_reply.started":"2024-10-14T22:18:26.690210Z","shell.execute_reply":"2024-10-14T22:18:26.695327Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-14T22:18:26.905263Z","iopub.execute_input":"2024-10-14T22:18:26.906002Z","iopub.status.idle":"2024-10-14T22:18:26.911018Z","shell.execute_reply.started":"2024-10-14T22:18:26.905958Z","shell.execute_reply":"2024-10-14T22:18:26.909751Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n\n    TransformerEncoderLayer can handle either traditional torch.tensor inputs,\n    or Nested Tensor inputs.  Derived classes are expected to similarly accept\n    both input formats.  (Not all combinations of inputs are currently\n    supported by TransformerEncoderLayer while Nested Tensor is in prototype\n    state.)\n\n    If you are implementing a custom layer, you may derive it either from\n    the Module or TransformerEncoderLayer class.  If your custom layer\n    supports both torch.Tensors and Nested Tensors inputs, make its\n    implementation a derived class of TransformerEncoderLayer. If your custom\n    Layer supports only torch.Tensor inputs, derive its implementation from\n    Module.\n\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of the intermediate layer, can be a string\n            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n            operations, respectively. Otherwise it's done after. Default: ``False`` (after).\n        bias: If set to ``False``, ``Linear`` and ``LayerNorm`` layers will not learn an additive\n            bias. Default: ``True``.\n\n    Examples::\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n        >>> src = torch.rand(10, 32, 512)\n        >>> out = encoder_layer(src)\n\n    Alternatively, when ``batch_first`` is ``True``:\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n        >>> src = torch.rand(32, 10, 512)\n        >>> out = encoder_layer(src)\n\n    Fast path:\n        forward() will use a special optimized implementation described in\n        `FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`_ if all of the following\n        conditions are met:\n\n        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor\n          argument ``requires_grad``\n        - training is disabled (using ``.eval()``)\n        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)\n        - activation is one of: ``\"relu\"``, ``\"gelu\"``, ``torch.functional.relu``, or ``torch.functional.gelu``\n        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed\n        - if src is a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_, neither ``src_mask``\n          nor ``src_key_padding_mask`` is passed\n        - the two ``LayerNorm`` instances have a consistent ``eps`` value (this will naturally be the case\n          unless the caller has manually modified one without modifying the other)\n\n        If the optimized implementation is in use, a\n        `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be\n        passed for ``src`` to represent padding more efficiently than using a padding\n        mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ will be\n        returned, and an additional speedup proportional to the fraction of the input that\n        is padding can be expected.\n\n        .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n         https://arxiv.org/abs/2205.14135\n\n    \"\"\"\n\n    __constants__ = ['norm_first']\n\n    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n                 bias: bool = True, device=None, dtype=None, num_experts: int = 8, num_experts_per_tok: int = 2) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout,\n                                            bias=bias, batch_first=batch_first,\n                                            **factory_kwargs)\n        \n        # Implementation of Feedforward model\n        self.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n        self.dropout = Dropout(dropout)\n        self.linear2 = Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n\n        self.norm_first = norm_first\n        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n\n        #creating the experts - \n        self.experts = [expert(d_model , dim_feedforward, bias) for i in range(num_experts)]\n        #creating gatingnetwork\n        self.gatingNetwork = GatingNetwork(input_dim , num_experts)\n        self.num_experts_per_tok = num_experts_per_tok\n        # Legacy string support for activation function.\n        if isinstance(activation, str):\n            activation = _get_activation_fn(activation)\n\n        # We can't test self.activation in forward() in TorchScript,\n        # so stash some information about it instead.\n        if activation is F.relu or isinstance(activation, torch.nn.ReLU):\n            self.activation_relu_or_gelu = 1\n        elif activation is F.gelu or isinstance(activation, torch.nn.GELU):\n            self.activation_relu_or_gelu = 2\n        else:\n            self.activation_relu_or_gelu = 0\n        self.activation = activation\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if not hasattr(self, 'activation'):\n            self.activation = F.relu\n\n\n    def forward(\n            self,\n            src: Tensor,\n            src_mask: Optional[Tensor] = None,\n            src_key_padding_mask: Optional[Tensor] = None,\n            is_causal: bool = False) -> Tensor:\n        r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n            is_causal: If specified, applies a causal mask as ``src mask``.\n                Default: ``False``.\n                Warning:\n                ``is_causal`` provides a hint that ``src_mask`` is the\n                causal mask. Providing incorrect hints can result in\n                incorrect execution, including forward and backward\n                compatibility.\n\n        Shape:\n            see the docs in :class:`~torch.nn.Transformer`.\n        \"\"\"\n        src_key_padding_mask = F._canonical_mask(\n            mask=src_key_padding_mask,\n            mask_name=\"src_key_padding_mask\",\n            other_type=F._none_or_dtype(src_mask),\n            other_name=\"src_mask\",\n            target_type=src.dtype\n        )\n\n        src_mask = F._canonical_mask(\n            mask=src_mask,\n            mask_name=\"src_mask\",\n            other_type=None,\n            other_name=\"\",\n            target_type=src.dtype,\n            check_other=False,\n        )\n\n        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n        x = src\n        if self.norm_first:\n            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n            x = x + self._ff_block(self.norm2(x))\n        else:\n            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))\n            x = self.norm2(x + self._ff_block(x))\n\n        return x\n\n\n    # self-attention block\n    def _sa_block(self, x: Tensor,\n                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n        x = self.self_attn(x, x, x,\n                           attn_mask=attn_mask,\n                           key_padding_mask=key_padding_mask,\n                           need_weights=False, is_causal=is_causal)[0]\n        return self.dropout1(x)\n\n    # feed forward block\n    def _ff_block(self, x: Tensor , num_experts_per_tok) -> Tensor:\n        gating_scores = F.softmax(self.gatingnetwork(x) , dim = -1)\n        top_scores, top_indices = gating_scores.topk(self.num_experts_per_tok , dim = -1 , sorted = False) \n        \n        top_mask = torch.zeros_like(gating_scores)\n        top_mask = top_mask.scatter(-1, top_indices, 1)\n        output = []\n        \n        for i in range(len(top_mask)):\n            if(top_mask[i] == 1):\n                output.append(top_scores[i]*self.experts[i](x))\n        \n        result = torch.sum(torch.stack(output) , dim = 0)\n        return result\n#         x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n#         return self.dropout2(x)\n\n\n    class GatingNetwork(nn.Module):\n        def __init__(self, input_dim, num_experts):\n            super(GatingNetwork, self).__init__()\n            self.gate = nn.Linear(input_dim, num_experts)\n\n        def forward(self, x):\n            return F.softmax(self.gate(x), dim=-1)\n    class expert(nn.Module):\n        def __init__(self,d_model , dim_feedforward , bias):\n            factory_kwargs = {'device': device, 'dtype': dtype}\n            self.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n            self.dropout = Dropout(dropout)\n            self.linear2 = Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n            self.dropout2 = Dropout(dropout)\n        def forward(self ,x):\n            x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n            return self.dropout2(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T22:21:31.759455Z","iopub.execute_input":"2024-10-14T22:21:31.759969Z","iopub.status.idle":"2024-10-14T22:21:31.795731Z","shell.execute_reply.started":"2024-10-14T22:21:31.759922Z","shell.execute_reply":"2024-10-14T22:21:31.794497Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# \"\"\"\n# This model integrates the MoE concept within a Transformer architecture. Each token's\n# representation is processed by a subset of experts, determined by the gating mechanism.\n# This architecture allows for efficient and specialized handling of different aspects of the\n# data, aiming for the adaptability and efficiency noted in the Mixtral 8x7B model's design\n# philosophy. The model activates only a fraction of the available experts for each token,\n# significantly reducing the computational resources needed compared to activating all experts\n# for all tokens.\n# \"\"\"\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# # Define the Expert class\n# class Expert(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, output_dim):\n#         super(Expert, self).__init__()\n#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n#     def forward(self, x):\n#         x = F.relu(self.fc1(x))\n#         return self.fc2(x)\n\n# # Define the Gating Network class\n# class GatingNetwork(nn.Module):\n#     def __init__(self, input_dim, num_experts):\n#         super(GatingNetwork, self).__init__()\n#         self.gate = nn.Linear(input_dim, num_experts)\n\n#     def forward(self, x):\n#         return F.softmax(self.gate(x), dim=-1)\n\n# # Define the Mixture of Experts Layer class\n# class MoELayer(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, output_dim, num_experts):\n#         super(MoELayer, self).__init__()\n#         self.experts = nn.ModuleList([Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)])\n#         self.gate = GatingNetwork(input_dim, num_experts)\n\n#     def forward(self, x, num_experts_per_tok):\n#         gating_scores = self.gate(x)\n#         topk_gating_scores, topk_indices = gating_scores.topk(num_experts_per_tok, dim=2, sorted=False)\n#         # Create a mask to zero out the contributions of non-topk experts\n#         mask = torch.zeros_like(gating_scores).scatter_(2, topk_indices, 1)\n#         # Use the mask to retain only the topk gating scores\n#         gating_scores = gating_scores * mask\n#         # Normalize the gating scores to sum to 1 across the selected top experts\n#         gating_scores = F.normalize(gating_scores, p=1, dim=2)\n        \n#         expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)\n#         expert_outputs = expert_outputs.transpose(1, 2)\n#         output = torch.einsum('bte,bteo->bto', gating_scores, expert_outputs)\n#         return output\n\n# # Define the overall Transformer model with integrated MoE\n# class TransformerWithMoE(nn.Module):\n#     def __init__(self, num_layers, dim, head_dim, hidden_dim, n_heads, num_experts, vocab_size, num_experts_per_tok):\n#         super(TransformerWithMoE, self).__init__()\n#         self.num_experts_per_tok = num_experts_per_tok\n#         self.embedding = nn.Embedding(vocab_size, dim)\n#         self.layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=dim, nhead=n_heads) for _ in range(num_layers)])\n#         self.moe_layer = MoELayer(dim, hidden_dim, dim, num_experts)\n#         self.output_layer = nn.Linear(dim, vocab_size)\n\n#     def forward(self, x):\n#         x = self.embedding(x)\n#         print('after embedding',x.shape)\n#         for layer in self.layers:\n#             print('before layers', x.shape)\n#             x = layer(x)\n#         print('after all layers', x.shape)\n#         x = self.moe_layer(x, self.num_experts_per_tok)\n#         logits = self.output_layer(x)\n#         return logits\n\n# # Initialize the model with configurations matching Mixtral 8x7B\n# model = TransformerWithMoE(\n#     num_layers=2,              # Number of transformer layers\n#     dim=4096,                   # Dimension of the model\n#     head_dim=128,               # Dimension of each head in the multi-head attention mechanisms\n#     hidden_dim=146,           # Hidden dimensionality in the feed-forward network within the transformer\n#     n_heads=32,                 # Number of attention heads\n#     num_experts=8,              # Number of experts in the MoE layer\n#     vocab_size=32000,           # Vocabulary size for the embedding layer\n#     num_experts_per_tok=2       # Number of experts activated per token\n# )\n#model(torch.randint(10,(40,)))","metadata":{"execution":{"iopub.status.busy":"2024-10-14T21:02:57.413492Z","iopub.execute_input":"2024-10-14T21:02:57.414200Z","iopub.status.idle":"2024-10-14T21:03:02.180434Z","shell.execute_reply.started":"2024-10-14T21:02:57.414146Z","shell.execute_reply":"2024-10-14T21:03:02.179230Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}