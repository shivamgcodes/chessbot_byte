{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:16.904327Z",
     "iopub.status.busy": "2024-10-23T20:20:16.903617Z",
     "iopub.status.idle": "2024-10-23T20:20:16.910970Z",
     "shell.execute_reply": "2024-10-23T20:20:16.909725Z",
     "shell.execute_reply.started": "2024-10-23T20:20:16.904278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Optional, Union, Callable\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#action-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:16.913465Z",
     "iopub.status.busy": "2024-10-23T20:20:16.913055Z",
     "iopub.status.idle": "2024-10-23T20:20:16.953842Z",
     "shell.execute_reply": "2024-10-23T20:20:16.952601Z",
     "shell.execute_reply.started": "2024-10-23T20:20:16.913425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.gate(x), dim=-1)\n",
    "\n",
    "\n",
    "class expert(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, bias, dropout, activation):\n",
    "        super().__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward,\n",
    "                              bias=bias, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model,\n",
    "                              bias=bias, **factory_kwargs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class google_expert(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, bias=True,):\n",
    "        super().__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.linar1 = nn.Linear(d_model, dim_feedforward,\n",
    "                                bias=bias, **factory_kwargs)\n",
    "        self.linear2 = nn.Linear(\n",
    "            d_model, dim_feedforward, bias=bias, **factory_kwargs)\n",
    "        self.linear_out = nn.Linear(\n",
    "            dim_feedforward, d_model, bias, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.linear1(x)\n",
    "        x2 = self.linear2(x)\n",
    "        x = F.silu(x1)*x2\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    __constants__ = ['norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 bias: bool = True, device=None, dtype=None, num_experts: int = 8, num_experts_per_tok: int = 2) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout,\n",
    "                                            bias=bias, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward,\n",
    "                              bias=bias, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model,\n",
    "                              bias=bias, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps,\n",
    "                               bias=bias, **factory_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps,\n",
    "                               bias=bias, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # creating the experts -\n",
    "        self.experts = nn.ModuleList(\n",
    "            [expert(d_model, dim_feedforward, bias) for i in range(num_experts)])\n",
    "\n",
    "        # creating gatingnetwork\n",
    "        self.gatingNetwork = GatingNetwork(d_model, num_experts)\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        # Legacy string support for activation function.\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            activation = _get_activation_fn(activation)\n",
    "\n",
    "        # We can't test self.activation in forward() in TorchScript,\n",
    "        # so stash some information about it instead.\n",
    "        if activation is F.relu or isinstance(activation, torch.nn.ReLU):\n",
    "            self.activation_relu_or_gelu = 1\n",
    "        elif activation is F.gelu or isinstance(activation, torch.nn.GELU):\n",
    "            self.activation_relu_or_gelu = 2\n",
    "        else:\n",
    "            self.activation_relu_or_gelu = 0\n",
    "        self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'activation'):\n",
    "            self.activation = F.relu\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            src: Tensor,\n",
    "            src_mask: Optional[Tensor] = None,\n",
    "            src_key_padding_mask: Optional[Tensor] = None,\n",
    "            is_causal: bool = False) -> Tensor:\n",
    "\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(src_mask),\n",
    "            other_name=\"src_mask\",\n",
    "            target_type=src.dtype\n",
    "        )\n",
    "\n",
    "        src_mask = F._canonical_mask(\n",
    "            mask=src_mask,\n",
    "            mask_name=\"src_mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), src_mask,\n",
    "                                   src_key_padding_mask, is_causal=is_causal)\n",
    "            x = x + self._ff_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, src_mask,\n",
    "                           src_key_padding_mask, is_causal=is_causal))\n",
    "            x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # self-attention block\n",
    "\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False, is_causal=is_causal)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # feed forward block\n",
    "#     def _ff_block(self, x: Tensor , num_experts_per_tok) -> Tensor:\n",
    "#         gating_scores = F.softmax(self.gatingNetwork(x) , dim = -1)\n",
    "#         top_scores, top_indices = gating_scores.topk(self.num_experts_per_tok , dim = -1 , sorted = False)\n",
    "\n",
    "#         top_mask = torch.zeros_like(gating_scores)\n",
    "#         top_mask = top_mask.scatter(-1, top_indices, 1)\n",
    "#         output = []\n",
    "\n",
    "#         for i in range(len(top_mask)):\n",
    "#             if(top_mask[i] == 1):\n",
    "#                 output.append(top_scores[i]*self.experts[i](x))\n",
    "\n",
    "#         result = torch.sum(torch.stack(output) , dim = 0)\n",
    "#         return result\n",
    "# #         x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "# #         return self.dropout2(x)hat\n",
    "\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        gating_scores = F.softmax(self.gatingNetwork(x), dim=-1)\n",
    "        top_scores, top_indices = gating_scores.topk(\n",
    "            self.num_experts_per_tok, dim=-1, sorted=False)\n",
    "\n",
    "        output = torch.zeros_like(x)\n",
    "\n",
    "        for i, index in enumerate(top_indices):\n",
    "            expert_outputs = [top_scores[i][j] * self.experts[index[j]]\n",
    "                              (x[i].unsqueeze(0)) for j in range(self.num_experts_per_tok)]\n",
    "            output[i] = sum(expert_outputs)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:17.022592Z",
     "iopub.status.busy": "2024-10-23T20:20:17.022112Z",
     "iopub.status.idle": "2024-10-23T20:20:17.033347Z",
     "shell.execute_reply": "2024-10-23T20:20:17.031940Z",
     "shell.execute_reply.started": "2024-10-23T20:20:17.022533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:17.037476Z",
     "iopub.status.busy": "2024-10-23T20:20:17.036579Z",
     "iopub.status.idle": "2024-10-23T20:20:17.054742Z",
     "shell.execute_reply": "2024-10-23T20:20:17.053361Z",
     "shell.execute_reply.started": "2024-10-23T20:20:17.037417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:17.057140Z",
     "iopub.status.busy": "2024-10-23T20:20:17.056706Z",
     "iopub.status.idle": "2024-10-23T20:20:17.078011Z",
     "shell.execute_reply": "2024-10-23T20:20:17.076813Z",
     "shell.execute_reply.started": "2024-10-23T20:20:17.057101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:22:46.086969Z",
     "iopub.status.busy": "2024-10-23T20:22:46.086368Z",
     "iopub.status.idle": "2024-10-23T20:22:46.095346Z",
     "shell.execute_reply": "2024-10-23T20:22:46.094004Z",
     "shell.execute_reply.started": "2024-10-23T20:22:46.086919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_fen = 'r2q1rk1/1bp2pbp/1p1p1npB/p1n1p3/P3P3/2NP1N1P/1PP1BPP1/R1Q2RK1 w - - 1 12'\n",
    "print(tokenize(sample_fen))\n",
    "print(len(tokenize(sample_fen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:17.100231Z",
     "iopub.status.busy": "2024-10-23T20:20:17.099711Z",
     "iopub.status.idle": "2024-10-23T20:20:37.049259Z",
     "shell.execute_reply": "2024-10-23T20:20:37.047441Z",
     "shell.execute_reply.started": "2024-10-23T20:20:17.100184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install python-chess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:37.052008Z",
     "iopub.status.busy": "2024-10-23T20:20:37.051556Z",
     "iopub.status.idle": "2024-10-23T20:20:37.215606Z",
     "shell.execute_reply": "2024-10-23T20:20:37.214293Z",
     "shell.execute_reply.started": "2024-10-23T20:20:37.051959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "_CHESS_FILE = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "def _compute_all_possible_actions() -> tuple[dict[str, int], dict[int, str]]:\n",
    "  \"\"\"Returns two dicts converting moves to actions and actions to moves.\n",
    "\n",
    "  These dicts contain all possible chess moves.\n",
    "  \"\"\"\n",
    "  all_moves = []\n",
    "\n",
    "  # First, deal with the normal moves.\n",
    "  # Note that this includes castling, as it is just a rook or king move from one\n",
    "  # square to another.\n",
    "  board = chess.BaseBoard.empty()\n",
    "  for square in range(64):\n",
    "    next_squares = []\n",
    "\n",
    "    # Place the queen and see where it attacks (we don't need to cover the case\n",
    "    # for a bishop, rook, or pawn because the queen's moves includes all their\n",
    "    # squares).\n",
    "    board.set_piece_at(square, chess.Piece.from_symbol('Q'))\n",
    "    next_squares += board.attacks(square)\n",
    "\n",
    "    # Place knight and see where it attacks\n",
    "    board.set_piece_at(square, chess.Piece.from_symbol('N'))\n",
    "    next_squares += board.attacks(square)\n",
    "    board.remove_piece_at(square)\n",
    "\n",
    "    for next_square in next_squares:\n",
    "      all_moves.append(\n",
    "          chess.square_name(square) + chess.square_name(next_square)\n",
    "      )\n",
    "\n",
    "  # Then deal with promotions.\n",
    "  # Only look at the last ranks.\n",
    "  promotion_moves = []\n",
    "  for rank, next_rank in [('2', '1'), ('7', '8')]:\n",
    "    for index_file, file in enumerate(_CHESS_FILE):\n",
    "      # Normal promotions.\n",
    "      move = f'{file}{rank}{file}{next_rank}'\n",
    "      promotion_moves += [(move + piece) for piece in ['q', 'r', 'b', 'n']]\n",
    "\n",
    "      # Capture promotions.\n",
    "      # Left side.\n",
    "      if file > 'a':\n",
    "        next_file = _CHESS_FILE[index_file - 1]\n",
    "        move = f'{file}{rank}{next_file}{next_rank}'\n",
    "        promotion_moves += [(move + piece) for piece in ['q', 'r', 'b', 'n']]\n",
    "      # Right side.\n",
    "      if file < 'h':\n",
    "        next_file = _CHESS_FILE[index_file + 1]\n",
    "        move = f'{file}{rank}{next_file}{next_rank}'\n",
    "        promotion_moves += [(move + piece) for piece in ['q', 'r', 'b', 'n']]\n",
    "  all_moves += promotion_moves\n",
    "\n",
    "  move_to_action, action_to_move = {}, {}\n",
    "  for action, move in enumerate(all_moves):\n",
    "    assert move not in move_to_action\n",
    "    move_to_action[move] = action\n",
    "    action_to_move[action] = move\n",
    "\n",
    "  return move_to_action, action_to_move\n",
    "\n",
    "\n",
    "MOVE_TO_ACTION, ACTION_TO_MOVE = _compute_all_possible_actions()\n",
    "NUM_ACTIONS = len(MOVE_TO_ACTION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T20:20:47.110526Z",
     "iopub.status.busy": "2024-10-23T20:20:47.110104Z",
     "iopub.status.idle": "2024-10-23T20:20:47.118447Z",
     "shell.execute_reply": "2024-10-23T20:20:47.116924Z",
     "shell.execute_reply.started": "2024-10-23T20:20:47.110488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(MOVE_TO_ACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-23T20:20:37.267121Z",
     "iopub.status.idle": "2024-10-23T20:20:37.267814Z",
     "shell.execute_reply": "2024-10-23T20:20:37.267515Z",
     "shell.execute_reply.started": "2024-10-23T20:20:37.267482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "class cust_embeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, emb_init_scale, use_sinosoidal=False, max_timescale=1e4):\n",
    "        super(cust_embeddings, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = 79\n",
    "        self.emb_init_scale = emb_init_scale\n",
    "        self.max_timescale = max_timescale\n",
    "        vocab_size = NUM_ACTIONS # same is done in the google repo , idk why though\n",
    "\n",
    "        if(use_sinosoidal):\n",
    "            pos_embeddings = self._get_sinusoidal_position_encoding(self.sequence_length, embedding_dim, max_timescale)\n",
    "            self.register_buffer(\"pos_embedding\", pos_embeddings)\n",
    "        else:\n",
    "            self.pos_embeddings = nn.Embedding(num_embeddings=self.sequence_length, embedding_dim=embedding_dim) #79 is the max seq length\n",
    "\n",
    "        self.embeddings_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        init.trunc_normal_(self.embeddings_layer.weight, std=self.emb_init_scale)\n",
    "    def _get_sinusoidal_position_encoding(self, sequence_length, embedding_dim, max_timescale):\n",
    "        \"\"\"\n",
    "        Creates sinusoidal encodings as in the original Transformer paper.\n",
    "\n",
    "        For each position pos and dimension i:\n",
    "          PE(pos, 2i)   = sin(pos / (max_timescale^(2i/embedding_dim)))\n",
    "          PE(pos, 2i+1) = cos(pos / (max_timescale^(2i/embedding_dim)))\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (sequence_length, embedding_dim).\n",
    "        \"\"\"\n",
    "        # Create a tensor of positions (shape: [sequence_length, 1])\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float32).unsqueeze(1)\n",
    "        # Compute the scaling term for each even index in the embedding dimensions.\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embedding_dim, 2, dtype=torch.float32) * (-math.log(max_timescale) / embedding_dim)\n",
    "        )\n",
    "        # Calculate the sinusoidal input by outer product (broadcast multiplication)\n",
    "        sinusoid_inp = position * div_term.unsqueeze(0)\n",
    "        pos_encoding = torch.zeros(sequence_length, embedding_dim)\n",
    "        pos_encoding[:, 0::2] = torch.sin(sinusoid_inp)\n",
    "        pos_encoding[:, 1::2] = torch.cos(sinusoid_inp)\n",
    "        return pos_encoding\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, \"pos_embeddings\"):\n",
    "            pos_embed  = self.pos_embeddings(torch.arange(self.sequence_length, device=x.device))\n",
    "        else:\n",
    "            pos_embed = self.pos_embeddings(torch.arange(self.sequence_length))\n",
    "        embed = self.embeddings_layer(x)*Math.sqrt(self.embedding_dim)\n",
    "        return pos_embed+embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def shift_right(sequences: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Right-shift the one-hot encoded input by padding on the temporal axis.\"\"\"    # assuming this is wrong , but bc google thori galat hoga\n",
    "    bos_array = torch.zeros((sequences.size(0), 1), dtype=torch.uint8)\n",
    "    # Concatenate the bos_array with sequences along the temporal axis\n",
    "    padded_sequences = torch.cat([bos_array, sequences], dim=1)\n",
    "    # Return the padded sequences, excluding the last element along the temporal axis\n",
    "    return padded_sequences[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, decoder_layernorm ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.use_layer_norm = decoder_layernorm\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    def forward(self, x):\n",
    "        if(self.use_layer_norm):\n",
    "            x = self.layer_norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class final_model(nn.Module):\n",
    "    def __init__(self , embedding_dim , encoder_layers, output_dim):\n",
    "        super(final_model, self).__init__()\n",
    "        self.embeddings = cust_embeddings(embedding_dim)\n",
    "        self.encoders = nn.ModuleList([TransformerEncoderLayer() for _ in range(encoder_layers)])\n",
    "        self.decoder = decoder(input_size=embedding_dim, output_size=output_dim, decoder_layernorm=True)\n",
    "        self.linear = nn.Linear(embedding_dim , output_dim)\n",
    "        \n",
    "    def forward (self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x)\n",
    "            \n",
    "        logits = self.decoder(x)\n",
    "    # Apply log softmax\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 723383,
     "sourceId": 1267593,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
